{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74919f46-3c5b-424a-8ba0-50de482ecdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /home/chandra.shekar/.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b594eb1d-16b5-42e2-b391-6c792555122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a24530-634f-4173-a3d4-484d25297d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854c1ed5-3ac0-4110-b373-09a4cf6730a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ùêêùêÆùêûùê¨ùê≠ùê¢ùê®ùêß\n",
    "# You are working as a Data Engineer on a big data processing pipeline, \n",
    "# where you need to analyze a large dataset from a retail business. \n",
    "# This dataset contains transaction information of customers, including a unique transaction ID, customer ID, product ID, transaction amount, and date. \n",
    "# The dataset has an index column, but you are required to include this index as a regular column in the DataFrame to perform some further transformations and joins. \n",
    "# Your task is to convert the index of the PySpark DataFrame into a column while preserving the dataset's integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524e614a-6076-4902-8c71-58dd3688918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 18:18:54 WARN Utils: Your hostname, MC1275 resolves to a loopback address: 127.0.1.1; using 192.168.29.179 instead (on interface wlp0s20f3)\n",
      "25/04/26 18:18:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/26 18:18:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/26 18:18:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/26 18:18:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/26 18:18:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/04/26 18:18:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+\n",
      "|Customer_ID|Product_ID|Amount|      Date|\n",
      "+-----------+----------+------+----------+\n",
      "|       1001|      5001|   200|2025-01-01|\n",
      "|       1002|      5002|   450|2025-01-02|\n",
      "|       1003|      5003|   300|2025-01-03|\n",
      "|       1004|      5004|   150|2025-01-04|\n",
      "|       1005|      5005|   500|2025-01-05|\n",
      "+-----------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "data = [ (1001, 5001, 200, \"2025-01-01\"), \n",
    "(1002, 5002, 450, \"2025-01-02\"), \n",
    "(1003, 5003, 300, \"2025-01-03\"), \n",
    "(1004, 5004, 150, \"2025-01-04\"), \n",
    "(1005, 5005, 500, \"2025-01-05\"), ] \n",
    "\n",
    "# Schema for DataFrame \n",
    "columns = [\"Customer_ID\", \"Product_ID\", \"Amount\", \"Date\"] \n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57017f9d-9968-43eb-8b8b-82c65e48ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 18:19:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Define a window (no partition, order by Date or any column that ensures order)\n",
    "windowSpec = Window.orderBy(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219695c2-1fa1-4998-a665-d5c6effefa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index column\n",
    "df_with_index = df.withColumn(\"Index_Column\", row_number().over(windowSpec) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef0dbbc3-a6d4-4a80-93c4-68c90bf3c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 18:19:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 18:19:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 18:19:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+------------+\n",
      "|Customer_ID|Product_ID|Amount|Date      |Index_Column|\n",
      "+-----------+----------+------+----------+------------+\n",
      "|1001       |5001      |200   |2025-01-01|0           |\n",
      "|1002       |5002      |450   |2025-01-02|1           |\n",
      "|1003       |5003      |300   |2025-01-03|2           |\n",
      "|1004       |5004      |150   |2025-01-04|3           |\n",
      "|1005       |5005      |500   |2025-01-05|4           |\n",
      "+-----------+----------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_index.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92d4fa3-52a8-4704-b4ad-6cf5d9852967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to SQL \n",
    "df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce753739-1314-4e8c-bc5e-52cff5215efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+\n",
      "|Customer_ID|Product_ID|Amount|      Date|\n",
      "+-----------+----------+------+----------+\n",
      "|       1001|      5001|   200|2025-01-01|\n",
      "|       1002|      5002|   450|2025-01-02|\n",
      "|       1003|      5003|   300|2025-01-03|\n",
      "|       1004|      5004|   150|2025-01-04|\n",
      "|       1005|      5005|   500|2025-01-05|\n",
      "+-----------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6030449d-a07b-4d75-8204-015032d024e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 19:02:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 19:02:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 19:02:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+------------+\n",
      "|Customer_ID|Product_ID|Amount|      Date|Index_column|\n",
      "+-----------+----------+------+----------+------------+\n",
      "|       1001|      5001|   200|2025-01-01|           0|\n",
      "|       1002|      5002|   450|2025-01-02|           1|\n",
      "|       1003|      5003|   300|2025-01-03|           2|\n",
      "|       1004|      5004|   150|2025-01-04|           3|\n",
      "|       1005|      5005|   500|2025-01-05|           4|\n",
      "+-----------+----------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT \n",
    "            *, ROW_NUMBER() OVER (ORDER by DATE) - 1 as Index_column\n",
    "            \n",
    "            FROM customers\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c318b-2827-43c3-a14d-bfaa9d58f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

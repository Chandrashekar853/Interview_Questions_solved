{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7c6d5b-e645-42b1-86e6-323c210fb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /home/chandra.shekar/.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a54e4ee-1808-40fe-98b3-72b45858c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487fa14c-3ed1-4fa7-b5d4-ffe1943b4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, sum, rank\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de3eb81-94a7-417a-8b7b-b24b6b42ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ùêêùêÆùêûùê¨ùê≠ùê¢ùê®ùêß\n",
    "# Imagine you're analyzing the monthly sales performance of a company across different regions. You want to calculate:\n",
    "# The cumulative sales for each region over months.\n",
    "# The rank of each month based on sales within the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d05ee2-cb57-4d5b-854b-c61c6a6d855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 19:30:37 WARN Utils: Your hostname, MC1275 resolves to a loopback address: 127.0.1.1; using 192.168.29.179 instead (on interface wlp0s20f3)\n",
      "25/04/26 19:30:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/26 19:30:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/26 19:30:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/26 19:30:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/26 19:30:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/04/26 19:30:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/04/26 19:30:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Region|Month|Sales|\n",
      "+------+-----+-----+\n",
      "|  East|  Jan|  200|\n",
      "|  East|  Feb|  300|\n",
      "|  East|  Mar|  250|\n",
      "|  West|  Jan|  400|\n",
      "|  West|  Feb|  350|\n",
      "|  West|  Mar|  450|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 19:30:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('monthly sales performance').getOrCreate()\n",
    "columns = [\"Region\", \"Month\", \"Sales\"]\n",
    "\n",
    "data = [ (\"East\", \"Jan\", 200), (\"East\", \"Feb\", 300), \n",
    "(\"East\", \"Mar\", 250), (\"West\", \"Jan\", 400), \n",
    "(\"West\", \"Feb\", 350), (\"West\", \"Mar\", 450) ]\n",
    "sales_df = spark.createDataFrame(data, columns)\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe77924-3a7c-4f4f-aa73-a674e6edd835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----------------+----+\n",
      "|Region|Month|Sales|cumulative_sales|Rank|\n",
      "+------+-----+-----+----------------+----+\n",
      "|  East|  Jan|  200|             200|   1|\n",
      "|  East|  Mar|  250|             450|   2|\n",
      "|  East|  Feb|  300|             750|   3|\n",
      "|  West|  Feb|  350|             350|   1|\n",
      "|  West|  Jan|  400|             750|   2|\n",
      "|  West|  Mar|  450|            1200|   3|\n",
      "+------+-----+-----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define a window partition by region and ordered by sales\n",
    "window_spec = Window.partitionBy(\"Region\").orderBy(\"Sales\")\n",
    "# Add cumulative sum and  rank column\n",
    "result_df = sales_df.withColumn(\"cumulative_sales\", sum(\"Sales\").over(window_spec)).withColumn(\"Rank\", rank().over(window_spec))\n",
    "\n",
    "result_df.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d17aec-4829-409b-8644-de884f35b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|Region|Month|Sales|\n",
      "+------+-----+-----+\n",
      "|  East|  Jan|  200|\n",
      "|  East|  Feb|  300|\n",
      "|  East|  Mar|  250|\n",
      "|  West|  Jan|  400|\n",
      "|  West|  Feb|  350|\n",
      "|  West|  Mar|  450|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Solutions\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "\n",
    "spark.sql(\"SELECT * FROM sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c723641c-d6bf-4a24-8d60-8195fe860dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----------------+----+\n",
      "|Region|Month|Sales|cumulative_sales|Rank|\n",
      "+------+-----+-----+----------------+----+\n",
      "|  East|  Jan|  200|             200|   1|\n",
      "|  East|  Mar|  250|             450|   2|\n",
      "|  East|  Feb|  300|             750|   3|\n",
      "|  West|  Feb|  350|             350|   1|\n",
      "|  West|  Jan|  400|             750|   2|\n",
      "|  West|  Mar|  450|            1200|   3|\n",
      "+------+-----+-----+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *,\n",
    "                SUM(Sales) OVER (PARTITION BY Region ORDER BY Sales) AS cumulative_sales,\n",
    "                RANK() OVER (PARTITION BY Region ORDER BY Sales) AS Rank\n",
    "        FROM Sales\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b578a-3e63-471d-be15-b465c3cf7e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
